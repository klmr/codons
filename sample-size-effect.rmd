```{r echo=FALSE}
modules::import('./scripts/knit', attach = TRUE)
opts_chunk$set(fig.path = 'figure/sample-size-effect-',
               cache.path = 'cache/sample-size-effect-',
               dev = c('png', 'pdf'))

config = modules::import('./config_human')
```

# The effect of sample size on properties of genes

Here we want to test how a certain feature of gene sets (in fact, the mean
codon usage, CU) is affected by random sampling. To test this, we sample
random sets of genes of different sizes and observe the effect this has on the
codon usage.

To make these results comparable with relevant data, we draw samples with
sizes equal to those of all GO categories (over some minimal size threshold).

**Caveat** In the following, we will use the codon usage, normalised by the
overall codon count (such that the values for all codons sum to 1). This is
done to make CU between different gene sets of different size comparable.
Initially I calculated the *relative* codon usage (RCU), which normalises each
codon frequency by the sum of the codon frequency of its synonymous codons. This
removes the amino acid usage bias as a confounder. However, it turns out that
this makes all RCUs of different gene sets highly correlated, since, across all
amino acids, codons with many synonymous codons will have lower values compared
to codons with few synonymous codons (simply because usage of synonymous codons
sums to 1).

```{r load-sampled-data, echo=FALSE}
# The sampling is *hellishly* slow – it took 230 CPU hours to complete.
# Consequently, we ran it on a cluster and load the results here, instead of
# actually recomputing them.
sampled_cu_fit = readRDS('./results/sampled-cu-fit.rds') %>%
    tidyr::gather(Replicate, Correlation, -Size) %>%
    mutate(Replicate = as.numeric(sub('^V', '', Replicate)))
```

Plot histograms for the distribution. Take out highest observations because they
are useless and skew everything.

```{r sampled-data-plot}
sampled_summary = sampled_cu_fit %>%
    group_by(Size) %>%
    do(.$Correlation %>% quantile(c(0, 0.02, 0.05, 0.25, 0.5, 0.75, 0.98, 1)) %>%
       unclass() %>% as.data.frame() %>% add_rownames('Cut')) %>%
    select(Size, Cut, Correlation = 3) %>%
    tidyr::spread(Cut, Correlation)

# We extent whiskers outside by 1.5 IQR from the IQR. This is a simplification
# which is aceptable here since the data is expected to be dense around that
# point so searching for an infimum/supremum brings little added accuracy.
ggplot(sampled_summary) +
    geom_segment(aes(x = Size, y = `25%`,
                     xend = Size, yend = `75%`),
                 color = '#808080') +
    geom_point(aes(x = Size, y = `2%`), size = 1)  +
    geom_point(aes(x = Size, y = `50%`), size = 1.5) +
    geom_point(aes(x = Size, y = `98%`), size = 1)  +
    scale_x_log10(name = 'Gene set size',
                  breaks = c(50, 100, 500, 1000, 5000, 10000)) +
    scale_y_log10(name = 'Codon usage correlation',
                  breaks = seq(0.7, 1, by = 0.1), limits = c(0.7, 1))
```

Now calculate the GO term CU and its correlation with the genomic CU.

```{r go-cu-cor}
sampling = import('sample-codon-usage')
sampling$load_codon_usage_data()
data = sampling$data
# Index gene name => `codon_usage` row index, necessary for lookup.
gene_index = data.frame(Gene = data$codon_usage[seq_len(data$stride), ]$Gene,
                        Index = seq_len(data$stride))

go_gene_index = inner_join(data$go_genes, gene_index, by = 'Gene')

# Since a join over > 1M rows becomes prohibitively slow, we use a loop +
# `filter` here instead.
go_gene_ids = unique(go_gene_index$GO)
go_gene_sizes = data$go_genes %>%
    select(GO, Size) %>%
    distinct(GO)

go_cu_fit = function (go_id)
    sampling$cu_fit(filter(go_gene_index, GO == go_id)$Index)

# (Redefine in case chunk above wasn’t run.)
cores = parallel::detectCores()

go_cu_cor = parallel::mclapply(go_gene_ids, go_cu_fit, mc.cores = cores) %>%
    {data.frame(GO = go_gene_ids, Correlation = unlist(.))} %>%
    inner_join(go_gene_sizes, by = 'GO') %>%
    inner_join(sampled_cu_fit, by = 'Size') %>%
    group_by(GO) %>%
    summarize(Correlation = first(Correlation.x),
              Size = first(Size),
              p = (sum(Correlation.y <= Correlation.x) + 1) / (n() + 1)) %>%
    mutate(padj = p.adjust(p, method = 'fdr'),
           Significant = padj < 0.05) %>%
    tbl_df()
```

```{r go-cu-fit-plot}
go_cu_cor %>%
    mutate(Truncated = Correlation < 0.7) %>%
ggplot(aes(x = Size, y = pmax(Correlation, 0.7))) +
    geom_line(aes(x = Size, y = `2%`), data = sampled_summary, size = 0.2) +
    geom_line(aes(x = Size, y = `98%`), data = sampled_summary, size = 0.2) +
    geom_point(aes(color = Significant, shape = Truncated), size = 2,
               show_guide = FALSE) +
    scale_shape_manual(limits = c(FALSE, TRUE), values = c(19, 6)) +
    scale_x_log10(name = 'GO term gene set size',
                  breaks = c(50, 100, 500, 1000, 5000, 10000)) +
    scale_y_continuous(name = 'Codon usage correlation',
                       breaks = seq(0.7, 1, 0.1), limits = c(0.7, 1)) +
    scale_color_manual(limits = c(FALSE, TRUE),
                       values = c('black', 'firebrick3'))
```

In total, `r sum(go_cu_cor$Significant)` out of `r nrow(go_cu_cor)` GO terms
(`r sprintf('%.0f%%', sum(go_cu_cor$Significant) / nrow(go_cu_cor) * 100)`)
show significantly more codon usage variation than expected by chance, at an
FDR-corrected significance cut-off of \(p < 0.05\).
